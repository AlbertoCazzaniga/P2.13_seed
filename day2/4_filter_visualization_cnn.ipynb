{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what convolutional networks learn\n",
    "\n",
    "\n",
    "![](./figs/alexnet_filters.png) \n",
    "\n",
    "\n",
    "Resources\n",
    "\n",
    "\n",
    "- Zeiler and Fergus paper \n",
    "\n",
    "  https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n",
    "\n",
    "- The building blocks of interpretability\n",
    "\n",
    "  https://distill.pub/2018/building-blocks/\n",
    "\n",
    "- A nice video from Yosinski et al.\n",
    "\n",
    "  https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=132s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract(self,x):\n",
    "        '''\n",
    "        Extract representations from input, first hidden layer and max pooling\n",
    "        '''\n",
    "        h0 = x\n",
    "        h1 = F.relu(self.conv1(x))\n",
    "        h2 = F.max_pool2d(h1, 2, 2)\n",
    "        return [h0,h1,h2]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "input_size=(1,28,28,)\n",
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "epochs=1\n",
    "lr=0.01\n",
    "momentum=0.0   \n",
    "seed=1\n",
    "log_interval=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided exercise\n",
    "\n",
    "We will now visualize the learned features in a convolutional network. \n",
    "\n",
    "We will\n",
    "\n",
    "- load the trained model\n",
    "- look once again at the model architecture \n",
    "  and focus on the first conv layer \n",
    "  \n",
    "  Question: how many filters/feature maps do we have? \n",
    "  \n",
    "- extract these filters\n",
    "\n",
    "Making sense of artificial units specialization in deeper layers typically\n",
    "require optimization.\n",
    "\n",
    "We will not cover it but you will find a nice example in Keras https://keras.io/examples/conv_filter_visualization/\n",
    "where, through an optimization process in input space you can\n",
    "visualize what neurons in deep layers (beyond the first convolutional layer)\n",
    "*prefer*.\n",
    "\n",
    "A picture here: these are the images preferred by some units\n",
    "\n",
    "![](./figs/vgg_block5_conv1.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = next(iter(test_loader))\n",
    "output = model(inputs)\n",
    "pred = output.argmax(dim=1, keepdim=True) \n",
    "correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "print('acc = {}'.format(correct/inputs.shape[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(np.squeeze(inputs[i]),cmap='bone')\n",
    "    plt.title(str(pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1_filters = list(model.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1_filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.imshow(np.squeeze(lay1_filters[i]),cmap='bone')\n",
    "    plt.title(i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract representations because we want to see how the network transformed one image at the first hidden layer.\n",
    "\n",
    "http://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[inp, conv1, maxpool1] = model.extract(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = conv1[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.imshow(x[i],cmap='bone')\n",
    "    plt.title(i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of filters in a *real* network: AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you will have to run\n",
    "\n",
    "    $ conda install ipywidgets\n",
    "    \n",
    "in your environemnt, if you experience an error in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "print('(The first time you download the pretrained model it will take a while...)')\n",
    "alex = alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in alex.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1_filters_alex = list(alex.parameters())[0].detach().numpy()\n",
    "print(lay1_filters_alex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "shape = np.squeeze(lay1_filters_alex[0]).shape\n",
    "new_shape = np.array(shape)\n",
    "new_shape = new_shape[[1,2,0]]\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(lay1_filters_alex[i].transpose(2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x -= np.min(x)\n",
    "    x /= np.max(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,20))\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.imshow(normalize(lay1_filters_alex[i].transpose(2,1,0)))\n",
    "    plt.title(i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

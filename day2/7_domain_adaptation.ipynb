{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain adaptation\n",
    "\n",
    "\n",
    "\n",
    "![](./figs/da.png)\n",
    "\n",
    "### Resources\n",
    "\n",
    "\n",
    "- The original paper \"Unsupervised Domain Adaptation by Backpropagation\" (2014)\n",
    "\n",
    "  http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf\n",
    "  \n",
    "  \n",
    "- Video of the Conference ICCV (2017)\n",
    "\n",
    "  In which the research is framed in a broader context\n",
    "\n",
    "  https://www.youtube.com/watch?v=uUUvieVxCMs&t=1210s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('da_results'):\n",
    "    os.makedirs('da_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import mnistm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayscaleToRgb:\n",
    "    \"\"\"Convert a grayscale image to rgb\"\"\"\n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)\n",
    "        image = np.dstack([image, image, image])\n",
    "        return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(x.shape[0], -1)\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/jvanvugt/pytorch-domain-adaptation/\n",
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "\n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10 #50\n",
    "seed = 1101\n",
    "batch_size = 64\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load('conv_for_domain_adaptation.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = model.feature_extractor\n",
    "clf = model.classifier\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    GradientReversal(),\n",
    "    nn.Linear(320, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_batch = batch_size // 2\n",
    "\n",
    "source_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           GrayscaleToRgb(),\n",
    "                           transforms.ToTensor(),                           \n",
    "                       ])),\n",
    "        batch_size=half_batch, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loader = torch.utils.data.DataLoader( \n",
    "    mnistm.MNISTM('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),                           \n",
    "                       ])), \n",
    "    batch_size=half_batch, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,labels = next(iter(target_loader))\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(imgs[19], (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(list(discriminator.parameters()) + list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    batches = zip(source_loader, target_loader)\n",
    "    n_batches = min(len(source_loader), len(target_loader))\n",
    "\n",
    "    total_domain_loss = total_label_accuracy = 0\n",
    "    for (source_x, source_labels), (target_x, _) in tqdm(batches, leave=False, total=n_batches):\n",
    "            x = torch.cat([source_x, target_x])\n",
    "            x = x.to(device)\n",
    "            domain_y = torch.cat([torch.ones(source_x.shape[0]),\n",
    "                                  torch.zeros(target_x.shape[0])])\n",
    "            domain_y = domain_y.to(device)\n",
    "            label_y = source_labels.to(device)\n",
    "\n",
    "            features = feature_extractor(x).view(x.shape[0], -1)\n",
    "            domain_preds = discriminator(features).squeeze()\n",
    "            label_preds = clf(features[:source_x.shape[0]])\n",
    "\n",
    "            domain_loss = F.binary_cross_entropy_with_logits(domain_preds, domain_y) #Ld\n",
    "            label_loss = F.cross_entropy(label_preds, label_y) #Ly\n",
    "            loss = domain_loss + label_loss\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_domain_loss += domain_loss.item()\n",
    "            total_label_accuracy += (label_preds.max(1)[1] == label_y).float().mean().item()\n",
    "\n",
    "    mean_loss = total_domain_loss / n_batches\n",
    "    mean_accuracy = total_label_accuracy / n_batches\n",
    "    tqdm.write(f'EPOCH {epoch:03d}: domain_loss={mean_loss:.4f}, '\n",
    "               f'source_accuracy={mean_accuracy:.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'da_results/revgrad_' + str(epoch) + '.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional work\n",
    "\n",
    "- Assess the performance of the model through all epochs on source and target dataset. You should define new loaders with a suitable batch size. Pay attention to the case of the source dataset. Plot the accuracy on source and target domains as a function of the epoch, including the 0 epoch, before domain adaptation.\n",
    "\n",
    "- Visualize features before and after domain adaptation with T-SNE. Is it visible the feature adaptation?\n",
    "\n",
    "- Compute the ID of source and target features beore and  during the domain adaptation. What do you expect to see?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
